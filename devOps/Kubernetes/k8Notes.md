# Kubernetes

Kubernetes runs your workload by placing containers into **Pods** to run on **Nodes**. *A node may be a virtual or physical machine*, depending on the cluster. Each node is managed by the **control plane** and contains the services necessary to run Pods.

> Services of an APP are containerized and turned into Pods.

## Architecture

The whole architecture of Kubernetes can be classified as 2 major components.

1. Worker nodes


2. Control Plane


![architecture](./Assets/components-of-kubernetes.svg)


### Worker nodes

Which host [Pods](#pods) to run the application workload. More specifically, the worker nodes manages **kubelets** and it is the **kubelets**, agents that run on each node in the cluster that makes sure that containers are running in a Pod. 


### [Control plane](https://kubernetes.io/docs/concepts/overview/components/#control-plane-components)

The control plane manages the worker nodes and the Pods in the cluster. It makes global decisions about the whole cluster. Therefore, there're normally multiple of such components for fault tolerance.

#### API server

The core of Kubernetes' control plane is the [API server](https://kubernetes.io/docs/concepts/overview/components/#kube-apiserver)

The API server exposes an HTTP API that lets end users, different parts of your cluster, and external components communicate with one another.


## Concepts

å¿…é¡»äº†è§£çš„ Kubernetes æ ¸å¿ƒæ¦‚å¿µ

Workloads

1. å·¥ä½œè´Ÿè½½ï¼ˆWorkloadsï¼‰

| èµ„æºç±»å‹    | ç”¨é€”                | é€‚ç”¨åœºæ™¯           |
| ----------- | ------------------- | ------------------ |
| Deployment  | æ— çŠ¶æ€åº”ç”¨          | WebæœåŠ¡ã€APIæœåŠ¡   |
| StatefulSet | æœ‰çŠ¶æ€åº”ç”¨          | æ•°æ®åº“ã€æœ‰çŠ¶æ€æœåŠ¡ |
| DaemonSet   | æ¯ä¸ªèŠ‚ç‚¹è¿è¡Œä¸€ä¸ªPod | æ—¥å¿—æ”¶é›†ã€ç›‘æ§ä»£ç† |
| Job/CronJob | æ‰¹å¤„ç†ä»»åŠ¡          | æ•°æ®å¤‡ä»½ã€å®šæ—¶ä»»åŠ¡ |

2. ç½‘ç»œï¼ˆNetworkingï¼‰

| èµ„æºç±»å‹      | ç”¨é€”                                     |
| ------------- | ---------------------------------------- |
| Service       | å†…éƒ¨æœåŠ¡å‘ç°å’Œè´Ÿè½½å‡è¡¡                   |
| Ingress       | HTTP/HTTPSè·¯ç”±ï¼ˆéœ€è¦Ingress Controllerï¼‰ |
| NetworkPolicy | ç½‘ç»œè®¿é—®æ§åˆ¶                             |

3. é…ç½®ï¼ˆConfigurationï¼‰

| èµ„æºç±»å‹  | ç”¨é€”                   |
| --------- | ---------------------- |
| ConfigMap | é…ç½®æ–‡ä»¶ã€ç¯å¢ƒå˜é‡     |
| Secret    | æ•æ„Ÿä¿¡æ¯ï¼ˆå¯†ç ã€å¯†é’¥ï¼‰ |

4. å­˜å‚¨ï¼ˆStorageï¼‰

| èµ„æºç±»å‹                    | ç”¨é€”         |
| --------------------------- | ------------ |
| PersistentVolume (PV)       | é›†ç¾¤å­˜å‚¨èµ„æº |
| PersistentVolumeClaim (PVC) | Podç”³è¯·å­˜å‚¨  |


ğŸ¯ å­¦ä¹ è·¯å¾„å»ºè®®

1. ç¬¬ä¸€é˜¶æ®µï¼ˆåŸºç¡€ï¼‰ï¼š

   1. Pod â†’ Deployment â†’ Service
   2. ConfigMap â†’ Secret
   3. åŸºæœ¬æ•…éšœæ’æŸ¥

2. ç¬¬äºŒé˜¶æ®µï¼ˆè¿›é˜¶ï¼‰ï¼š

   1. Ingress â†’ åŸŸåè®¿é—®
   2. PersistentVolume â†’ æ•°æ®æŒä¹…åŒ–
   3. Resource Limits â†’ èµ„æºç®¡ç†

3. ç¬¬ä¸‰é˜¶æ®µï¼ˆé«˜çº§ï¼‰ï¼š
   1. Helm â†’ åº”ç”¨æ‰“åŒ…
   2. HPA â†’ è‡ªåŠ¨æ‰©ç¼©å®¹
   3. NetworkPolicy â†’ ç½‘ç»œå®‰å…¨



### Node

A node may be a virtual or physical machine, depending on the cluster

### [Pods](https://kubernetes.io/docs/concepts/workloads/pods/)

A Pod (as in a pod of whales or pea pod) is a group of one or more containers, with shared storage and network resources, and a specification for how to run the containers.

**A Pod is not a process, but an environment for running container(s).**
Therefore, Restarting a container in a Pod should not be confused with restarting a Pod.

> A Pod is similar to **a set of containers with shared namespaces and shared filesystem volumes**

> In the majority of cases, Pods run a single container. The "one-container-per-Pod" model basically means a Pod is just a wrapper of a container.


#### Defining a pod

We can directly write a Pod yaml file to create as well as using cli to add the pod to the cluster. 

**However**, it's almost always to create pods using [workload resource api](#workload-resources), which is an abstract layer over Pods.

> to make life considerably easier, you don't need to manage each Pod directly. Instead, you can use workload resources that manage a set of pods on your behalf. These resources configure controllers that make sure the right number of the right kind of pod are running, to match the state you specified.

[**Naked Pods will not be rescheduled in the event of a node failure.**](https://kubernetes.io/docs/concepts/configuration/overview/#naked-pods-vs-replicasets-deployments-and-jobs)


https://kubernetes.io/docs/concepts/workloads/pods/#pod-templates


### [Workload Resources](https://kubernetes.io/docs/concepts/workloads/)

A higher abstraction level than a Pod, then the Kubernetes control plane automatically manages Pod objects on your behalf, based on the specification for the workload object you defined.

Kubernetes provides several built-in workload resources:

| èµ„æºç±»å‹    | ç”¨é€”                | é€‚ç”¨åœºæ™¯           |
| ----------- | ------------------- | ------------------ |
| Deployment  | æ— çŠ¶æ€åº”ç”¨          | WebæœåŠ¡ã€APIæœåŠ¡   |
| StatefulSet | æœ‰çŠ¶æ€åº”ç”¨          | æ•°æ®åº“ã€æœ‰çŠ¶æ€æœåŠ¡ |
| DaemonSet   | æ¯ä¸ªèŠ‚ç‚¹è¿è¡Œä¸€ä¸ªPod | æ—¥å¿—æ”¶é›†ã€ç›‘æ§ä»£ç† |
| Job/CronJob | æ‰¹å¤„ç†ä»»åŠ¡          | æ•°æ®å¤‡ä»½ã€å®šæ—¶ä»»åŠ¡ |



### Define Services

Define services before Workload Resources.

When Kubernetes starts a container, it provides environment variables pointing to all the Services which were running when the container was started. **any Service that a Pod wants to access must be created before the Pod itself,**

For clusterIP:

`port` is the listing port that the service itself listens to. (not defined will be random but matches the service port of an Ingress)
`targetPort` is the destination it should forward to.

> `targetPort` should match an open port of a pod.

For NodePort:

Cluster doesn't have an IP address, but nodes have. Therefore, NodePort Service **statically** assigns a port on **each worker node**. Also necessary bounding to clusterIP service is automatically created. So that We can use the **public** ip of a node and the port to access.

> You have to get a public ip for the node yourself, the node ip itself is private.




## [Volumes](https://www.youtube.com/watch?v=0swOh5C3OVM)

Kubernetes doesn't have data persistence out of box, a `mysql` database pod after restart will lose all its data. Therefore, we have volumes to persist


There are 3 major components of Kubernetes storage

1. Persistent Volume, 
2. Persistent Volume Claim
3. Storage Class


A reference: [Static vs. Dynamic Storage Provisioning: A Look Under the Hood](https://bluexp.netapp.com/blog/cvo-blg-static-vs.-dynamic-storage-provisioning-a-look-under-the-hood#h_h1)


### Persistent Volume 

PV is also a resource so that it can be defined using a yaml file. However, it serves purely as an interface of a storage to cluster. The actual backend storage is created / managed all by ourselves. Therefore, the backend can be a local storage, google cloud and so on. The available storage backend can be found at the Kubernetes websites.

**PV doesn't have namespaces. Therefore, its visible to the whole cluster**

> Therefore, we can regard PV as a representative of an actual storage.

> PVs live in a cluster and their are multiple ones controlled by the administrator of the cluster.


### Persistent Volume Claim

For a pod, or more specifically a container inside a pod to mount a PV. It has to send a request to the cluster to ask for the resource. Such requests are called Persistent Volume Claim. The PVC contains the number of storage that is required, and the cluster will go through all the available PV in the cluster that found a suitable one that can satisfy the claim.

### Storage Class

As mentioned in the [Persistent Volume](#persistent-volume), PV are resource manged by the administrators, and they're mounted to containers by PVC. However, this assumes among requesting a new PVC, there's an available and suitable PV in the cluster. 

To create a PV, the administrators have to **MANUALLY** create one with provisions. This is certainly undesirable. Therefore, to automate the process, administrators can declare Storage Classes with provisioners (the backend, e.g Google Cloud). So that when requested, PV can be dynamically created. 

### Service


In Kubernetes, a **Service** is a method for exposing a network application that is running as one or more Pods in your cluster.

> A single service is bound to one or more pods


Each Pod gets its own IP address (Kubernetes expects network plugins to ensure this). For a given Deployment in your cluster, the set of Pods running in one moment in time could be different from the set of Pods running that application a moment later.

This leads to a problem: if some set of Pods (call them "backends") provides functionality to other Pods (call them "frontends") inside your cluster, how do the frontends find out and keep track of which IP address to connect to, so that the frontend can use the backend part of the workload?

To solve this we can define a service `ClusterIP`

Service çš„ ClusterIP å°±æ˜¯ç»™å®ƒèƒŒåé‚£ä¸€ç¾¤ Pod èµ·äº†ä¸€ä¸ªã€ç»Ÿä¸€å…¥å£ã€ï¼Œé›†ç¾¤å†…åªè¦è®¿é—®è¿™ä¸ª IPï¼ˆæˆ–æ›´å¸¸è§çš„ DNS åï¼‰å°±èƒ½è½åˆ°å…¶ä¸­æŸä¸€ä¸ª Pod ä¸Š.

> Choosing `ClusterIP` makes the Service only reachable from within the cluster.





### Ingress


If your workload speaks HTTP, you might choose to use an Ingress to control how web traffic reaches that workload. Ingress is not a Service type, but it acts as the entry point for your cluster. **An Ingress lets you consolidate your routing rules into a single resource**, so that you can expose multiple components of your workload, running separately in your cluster, behind a single listener.


Regard an Ingress as a routing table, it defines with hostname maps to which service.

In order for the ingress to work, we need a ingress controller which is actually a pod doing the job to handle the routing. example controllers are `ingress-nginx`


```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: demo-ingress
  namespace: demo
  annotations:
    # ä¸‹é¢ä¸¤è¡Œåªæœ‰ç”¨ nginx-ingress æ—¶æ‰éœ€è¦ï¼Œå¯æŒ‰éœ€åˆ 
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  # æŒ‡å®šç”±å“ªä¸ª IngressClass å¤„ç†ï¼›é›†ç¾¤é‡Œå¿…é¡»å·²éƒ¨ç½²åŒå Controller
  ingressClassName: nginx
  rules:
  - host: demo.example.com      # æ”¹æˆæœ¬æœº hosts èƒ½è§£æçš„åŸŸå
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: nginx-svc
            port:
              number: 80
      - path: /api
        pathType: Prefix
        backend:
          service:
            name: whoami-svc
            port:
              number: 80
  # å¦‚æœæƒ³å¼ºåˆ¶ HTTPSï¼Œå¯å†é… tls: æ®µè½
```

Here, it defines `demo.example.com` maps to `nginx-svc` service. 

Here, it declares that `demo.example.com` should be routed to the Service named `nginx-svc`; it is simply telling the Ingress Controller, **â€œLook up the Endpoints behind nginx-svc and give me the list.â€**

> The Controller then forwards traffic **straight to those Pod IPs** at their target ports, never touching the ClusterIP.

Thus the Service acts only as a label-based finder plus port spec, not as a hop in the data path.



### Ingress Controller


Ingress Controller æœ¬èº«å°±æ˜¯ä¸€ä¸ªï¼ˆæˆ–ä¸€ç»„ï¼‰Podï¼›ç»™å®ƒå†é…ä¸€ä¸ª Service of type NodePort å°±ç­‰äºåœ¨è¿è¡Œå®ƒçš„æ¯ä¸ªèŠ‚ç‚¹ä¸Šéƒ½æŒ–äº†ä¸€ä¸ª 30000â€“32767 èŒƒå›´å†…çš„é«˜ç«¯å£ã€‚ä»»ä½•å®¢æˆ·ç«¯åªè¦è®¿é—® <ä»»æ„èŠ‚ç‚¹IP>:é‚£ä¸ªNodePort å°±èƒ½æŠŠæµé‡é€è¿› Controller Podï¼Œäºæ˜¯æ•´ä¸ª Ingress é€šè·¯å°±é€šäº†ã€‚

å½“Ingress Controllerè¢«è®¾ç½®ä¸ºLoadbalanceræ—¶ï¼Œasks the **cloud provider** to create an external cloud load balancerï¼Œ and
a) å‘å¤–ç½‘ç”³è¯·ä¸€ä¸ªçœŸæ­£çš„ äº‘è´Ÿè½½å‡è¡¡å™¨ï¼ˆAWS ELB/ALBã€GCP GLBã€é˜¿é‡Œäº‘ SLBâ€¦ï¼‰ï¼›
b) æŠŠäº‘ LB çš„åç«¯æ± æŒ‡å‘é›†ç¾¤é‡Œ æ‰€æœ‰èŠ‚ç‚¹ çš„ NodePortï¼ˆk8s ä¼šè‡ªåŠ¨åˆ›å»ºå¯¹åº”çš„ NodePort å³ä½¿ä½ æ²¡æ˜¾å¼å†™ï¼‰ï¼›
c) æŠŠäº‘ LB çš„å…¬ç½‘ IP å†™å› Service çš„ status.loadBalancer.ingress å­—æ®µã€‚



### kubectl

CLI that used to inspect and manage **cluster resources**, and view logs.

Under the hood it uses the [API server](#api-server).

### Minikube

lets you run Kubernetes **locally**. minikube runs an all-in-one or a multi-node local Kubernetes cluster on your personal computer. For testing purposes.

#### Commands


Apply Pods

```bash
kubectl apply -f path/to/pods.yaml
```

List all contexts. Context stores the user and cluster information used for authentication as well as controlling the resources. More details can be found [kubectl context vs cluster](https://stackoverflow.com/questions/56299440/kubectl-context-vs-cluster)

```bash
kubectl config get-contexts
```


## [Write Yaml](https://kubernetes.io/docs/concepts/overview/working-with-objects/#describing-a-kubernetes-object)

We can regard all the components of Kubernetes as objects, and use yaml file to declare objects.


### Service accounts

Service Accounts are used to authenticate **pods** when they need to interact with the Kubernetes API server or other cluster resources. This identity is useful in various situations, including authenticating to the API server or implementing identity-based security policies.

Here's how it works:

1. Pod Authentication: When a pod needs to make requests to the Kubernetes API server or access other resources within the cluster, it uses its associated Service Account for authentication.
2. Service Account Token: Each pod is mounted with a Service Account token at a specific path within the pod's filesystem. This token is a credential that allows the pod to authenticate with the Kubernetes API server.
3. **RBAC Permissions**: The permissions granted to a pod are determined by the Service Account's associated Role-Based Access Control (RBAC) policies. These policies define what resources the pod can access and what actions it can perform within the cluster.



## Production


### Installation

For a cluster you're managing yourself, the officially supported tool for deploying Kubernetes is [kubeadm](https://kubernetes.io/docs/setup/#production-environment).


#### [Disable Swap](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#swap-configuration)

```bash
sudo swapoff -a


# å¤‡ä»½ fstab
sudo cp /etc/fstab /etc/fstab.bak

# æ³¨é‡Šæ‰æ‰€æœ‰åŒ…å« swap çš„è¡Œ
sudo sed -i '/swap/s/^/#/' /etc/fstab
```


#### [Add official Kubernetes APT repository](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#installing-kubeadm-kubelet-and-kubectl)

```bash
sudo apt-get update
# apt-transport-https may be a dummy package; if so, you can skip that package
sudo apt-get install -y apt-transport-https ca-certificates curl gpg


# If the directory `/etc/apt/keyrings` does not exist, it should be created before the curl command, read the note below.
# sudo mkdir -p -m 755 /etc/apt/keyrings
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.34/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg

# This overwrites any existing configuration in /etc/apt/sources.list.d/kubernetes.list
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.34/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list
```


Install 

```bash
# kubectl is optional for worker nodes
sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl
# é”å®šç‰ˆæœ¬ï¼Œé˜²æ­¢è‡ªåŠ¨æ›´æ–°
sudo apt-mark hold kubelet kubeadm kubectl
```


#### [Install contianer runtime, specicially containerd](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#installing-runtime)


To install `containerd`

1. [Add Docker APT repository](https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository)

```bash
# Add Docker's official GPG key:
sudo apt-get update
sudo apt-get install ca-certificates curl
sudo install -m 0755 -d /etc/apt/keyrings
sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc
sudo chmod a+r /etc/apt/keyrings/docker.asc

# Add the repository to Apt sources:
echo \
  "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \
  $(. /etc/os-release && echo "${UBUNTU_CODENAME:-$VERSION_CODENAME}") stable" | \
  sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
sudo apt-get update
```

2. install and start containerd

```bash
sudo apt-get install containerd.io

sudo systemctl start containerd
sudo systemctl enable containerd
```

3. [configure](https://kubernetes.io/docs/setup/production-environment/container-runtimes/#install-and-configure-prerequisites)


1. enable `ipv4_forward`

```bash
# sysctl params required by setup, params persist across reboots
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.ipv4.ip_forward = 1
EOF

# Apply sysctl params without reboot
sudo sysctl --system
```

2. [load required kernel modules](https://www.plural.sh/blog/install-kubernetes-ubuntu-tutorial/)

> This is not mentioned in the official doc as kubeadm will not check this, but CNI `flannel` requires.

```bash
sudo modprobe overlay
sudo modprobe br_netfilter


cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF


cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF

sudo sysctl --system
```

3. set cgroup driver for kubelet and container runtime

> The `cgroupfs driver` is not recommended when `systemd` is the init system because `systemd` expects a single cgroup manager on the system.

```bash
# to test if systemd is the init system
ps -p 1 -o comm=
```

the cgroup drive of kubelet is default to systemd since v1.22, so we only have to configure container runtime to use the same driver

Follow https://kubernetes.io/docs/setup/production-environment/container-runtimes/#containerd

```bash
# (optional) consider resetting the /etc/containerd/config.toml first if the file is empty
containerd config default | sudo tee /etc/containerd/config.toml

# set SystemdCgroup = true
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
```


Both workder/master nodes need to Proceed the above steps

From this point, only master ndoes should perform.


#### START

1. can use `kubeadm config print` conmannd to print the current config files

```bash
kubeadm config print init-defaults > kubeadm-config.yaml
```

If you have plans to upgrade this single control-plane kubeadm cluster to high availability you should specify the `--control-plane-endpoin`t to set the shared endpoint for all control-plane nodes. 

which corresponds to `controlPlaneEndpoint` in the config file

> Turning a single control plane cluster created without --control-plane-endpoint into a highly available cluster is not supported by kubeadm.

### [Install **CNI** which is used for the communication between pods](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#pod-network)

3 popular CNI add-ons

include: 
1. Flannel
2. Calico
3. Cilium

ranking from easy to hard in term of learning curve and capability.



For Flannel


add the following to `/run/flannel/subnet.env` according to `https://github.com/kubernetes/kubernetes/issues/70202`

```bash
FLANNEL_NETWORK=10.244.0.0/16
FLANNEL_SUBNET=10.244.0.1/24
FLANNEL_MTU=1450
FLANNEL_IPMASQ=true
```

Also, add `podSubnet` under the network section in the init config file.

```yaml
networking:
    ...
    podSubnet: "10.244.0.0/16"
```


```bash
# start!
sudo kubeadm init --config=kubeadm-config.yaml
 
# Post init for kubectl
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

# kubectl should not be run as sudo
kubectl apply -f <add-on.yaml>
```


### Install Ingress Controller


```bash
helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
helm repo update
helm -n ingress-nginx install ingress-nginx ingress-nginx/ingress-nginx --create-namespace
```

### install MetalLB

If we define Ingress Controller to be type of `NodePort`, later when visiting, we have to define
domain name with port, which is inconnvinent, thus, a better approach is to define a LoadBalancer.

```bash
helm repo add metallb https://metallb.github.io/metallb
kubectl create namespace metallb-system
helm install metallb metallb/metallb
```

Also apply config for configuring ip address pool

```bash
cat > metallb-config.yaml << 'EOF'
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: default-pool
  namespace: metallb-system
spec:
  addresses:
  - 10.21.10.200-10.21.10.210  # ğŸ“ ä½¿ç”¨ä½ çš„èŠ‚ç‚¹ç½‘æ®µ
---
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: default-advertisement
  namespace: metallb-system
spec:
  ipAddressPools:
  - default-pool
EOF
```

> For the addresses, this has to be the same net for your local network, so that others can access.

```bash
kubectl apply -f metallb-config.yaml
```

#### Install Longhorn

For Storage, install https://longhorn.io/docs/1.10.1/deploy/install/install-with-helm/

Don't forget to use 


```bash
helm show values <release name>
```

to use get the confiration file, and update reclaim policy to `Retain`

```bash
reclaimPolicy: Retain
```

###### enable Longhorn UI

Enable xieIngress in `longhorn-values.yaml`

```yaml
ingress:
  # -- Setting that allows Longhorn to generate ingress records for the Longhorn UI service.
  enabled: true
  ingressClassName: nginx
  # -- Hostname of the Layer 7 load balancer.
  host: longhorn.cares-copilot.com
  ....
  annotations:
    # type of authentication
    nginx.ingress.kubernetes.io/auth-type: basic
    # prevent the controller from redirecting (308) to HTTPS
    nginx.ingress.kubernetes.io/ssl-redirect: 'false'
    # name of the secret that contains the user/password definitions
    nginx.ingress.kubernetes.io/auth-secret: basic-auth
    # message to display with an appropriate context why the authentication is required
    nginx.ingress.kubernetes.io/auth-realm: 'Authentication Required '
    # custom max body size for file uploading like backing image uploading
    nginx.ingress.kubernetes.io/proxy-body-size: 10000m

```

create Auth credential for admin using secret with name `basic-auth`, https://longhorn.io/docs/1.10.1/deploy/accessing-the-ui/longhorn-ingress/

```bash
$ USER=<USERNAME_HERE>; PASSWORD=<PASSWORD_HERE>; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth

# USER=longhorn_admin; PASSWORD=ujFkycnf1Awp; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth


kubectl -n longhorn-system create secret generic basic-auth --from-file=auth
```

#### Backups

connect Aliyun OSS for backup, configure in the `longhorn-values.yaml` the backupTarget to be `s3://<bucket_name>.<region>` according to [Backup Target Setup](https://longhorn.io/docs/1.10.1/snapshots-and-backups/backup-and-restore/set-backup-target/#set-up-aws-s3-backupstore)

```yaml
defaultBackupStore:
  # -- Endpoint used to access the default backupstore. (Options: "NFS", "CIFS", "AWS", "GCP", "AZURE")
  backupTarget: "s3://local-k8s-longhorn-backups.oss-cn-hongkong/"
  # -- Name of the Kubernetes secret associated with the default backup target.
  backupTargetCredentialSecret: aliyun-oss-access-credentials
  # -- Number of seconds that Longhorn waits before checking the default backupstore for new backups. The default value is "300". When the value is "0", polling is disabled.
  pollInterval: 300
```

Define `aliyun-oss-access-credentials` secret

```bash
kubectl create secret generic aliyun-oss-access-credentials   -n longhorn-system   --from-literal=AWS_ACCESS_KEY_ID=****
--from-literal=AWS_SECRET_ACCESS_KEY=****   
--from-literal=AWS_ENDPOINTS=https://s3.oss-cn-hongkong.aliyuncs.com  --from-literal=AWS_REGION=oss-cn-hongkong --from-literal=VIRTUAL_HOSTED_STYLE=true
```

since Aliyun uses virtual-hosted-addresss (the url schema is `s3://<butcket_name>.<region>` instead of `s3://<butcket_name>@<region>`), we also need to specify `VIRTUAL_HOSTED_STYLE=true` in secret.

#### reccurring jobs

Now, we have a backup target, to run the bakcup daily or monthly, we also need to setup a recurring job to perform such backups.

Through [UI](https://longhorn.io/docs/1.10.1/snapshots-and-backups/scheduling-backups-and-snapshots/), add jobs.

> leave `group` to default so that we can have all volumes without group specified to enjoy the recurring jobs.

> all volumes have to be healthy for the recurring backup job to work.



### Install gitea


```bash
helm repo add gitea-charts https://dl.gitea.com/charts/
kubectl create namespace gitea


helm install gitea gitea-charts/gitea -f gitea-values.yaml -n gitea
```

#### Upgrade from SingleNode to MultiNode Cluster


By default the `values.yaml` indicates the persistence of `gitea.shared.storage` to use `ReadWriteOnce` which means only a single pod can access the volume. This `RWO` only fits when the cluster has a single workder node. When adding a new node, and update the `replicaCount` in the `values.yaml` from 1 to 2, the second pod will not init correctly due to mounting error.


To fix this, either, we define `accessModes: - ReadWriteMany` in advance before initing gitea

```yaml
persistence:
  enabled: true
  create: true
  mount: true
  claimName: gitea-shared-storage
  size: 10Gi
  accessModes:
    - ReadWriteMany
  labels: {}
  storageClass: "longhorn"
```

Or for existing volumes using RWO, the access mode can't be changed once created, so we have to remount the volume and restore the data.

1. downgrade the replicaCount of existing gitea to 0

```bash
helm upgrade gitea gitea-charts/gitea -n gitea \
  --set replicaCount=0 -f gitea-values.yaml
```

> This will cause all pods to be deleted, but the volumes and pvcs are still reserved.

2. delete existing pvc

```bash
kubectl delete pvc gitea-shared-storage -n gitea
```

3. find the corresponding volume bounded to `gitea-shared-storage` pod in the longhorn web ui, click **restore** 
  > This will prompt to create a new volume based on the backup, in this case apart from the new volume name, we have to specify the access mode of the new volume to be `ReadWriteMany`

4. for the newly created `RWX` volume, which is supposed to be **detached**  at this moment, click **create PV/PVC**, this will prompt to enter the PVC name and namespace, specify `gitea-shared-storage` and `gitea`.

5. Now, everything should work, increase the `replicaCount` of the `gitea-values.yaml` back to 2 from 0 to restore the whole system.

### expose ssh

Since Ingress only works with http, while gitea push and many other operations replies on ssh, in the `values.yaml` we also need to specify  `type: LoadBalancer`.


```yaml
  ssh:
    type: LoadBalancer
    port: 22
    # clusterIP:
    loadBalancerIP:
    nodePort:
    externalTrafficPolicy:
    externalIPs:
    ipFamilyPolicy:
    ipFamilies:
    hostPort:
    loadBalancerSourceRanges: []
    annotations: {}
    labels: {}
    loadBalancerClass:
```

> When specifying `LoadBalancer`, k8s will automatically assign an external ip using the LoadBalancer provider of the cluster if has one, here, since we installed a `Metallb`, it works.

### Helm


[install helm](https://helm.sh/docs/intro/install/)


show helm default configs

```bash
helm show values <release name>
```


show current configs


```bash
helm get values <release name>
```

### Inter Connection

Now, services can be accessed by the external domain name specified by the ingress, however, the provided domain name can not be accessed by the cluster, for example, external users can use `git.cares-copilot.com` to access gitea, but the internal jenkins can't resolve this dns but to use `git-http:3000` which is internally available.

We can resolve this, by having a dns resolver externally that is accessed by the network, or we need to configure the `coredns` that is provided by the cluster.

```bash
# 1. æŸ¥çœ‹å½“å‰çš„ CoreDNS é…ç½®
kubectl get configmap -n kube-system coredns -o yaml

# 2. ç¼–è¾‘ CoreDNS é…ç½®ï¼Œæ·»åŠ å¤–éƒ¨åŸŸåè½¬å‘
kubectl edit configmap -n kube-system coredns
```

add dns resolution to hosts
```
  hosts {
    10.21.10.5 git.cares-copilot.com
    10.21.10.5 jenkins.cares-copilot.com
    10.21.10.5 longhorn.cares-copilot.com
    # æˆ–è€…ä½¿ç”¨é€šé…ç¬¦ï¼ˆå¦‚æœ CoreDNS ç‰ˆæœ¬æ”¯æŒï¼‰
    # 10.21.10.5 *.cares-copilot.com
    fallthrough
  }
```

The final config look like

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: coredns
  namespace: kube-system
data:
  Corefile: |
    .:53 {
        errors
        health {
           lameduck 5s
        }
        ready
        kubernetes cluster.local in-addr.arpa ip6.arpa {
           pods insecure
           fallthrough in-addr.arpa ip6.arpa
           ttl 30
        }
        # æ·»åŠ  hosts é…ç½®
        hosts {
          10.21.10.5 git.cares-copilot.com
          10.21.10.5 jenkins.cares-copilot.com
          10.21.10.5 longhorn.cares-copilot.com
          # æˆ–è€…ä½¿ç”¨é€šé…ç¬¦ï¼ˆå¦‚æœ CoreDNS ç‰ˆæœ¬æ”¯æŒï¼‰
          # 10.21.10.5 *.cares-copilot.com
          fallthrough
        }
        prometheus :9153
        forward . /etc/resolv.conf
        cache 30
        loop
        reload
        loadbalance
    }
```

To test it out

```bash
# å¯åŠ¨ä¸´æ—¶è°ƒè¯• Pod
kubectl run -it --rm debug --image=alpine --restart=Never -- sh

# å®‰è£…å·¥å…·
apk add curl

# æµ‹è¯•è§£æ
nslookup jenkins.cares-copilot.com
# åº”è¿”å› 10.21.10.5

# æµ‹è¯•è¿é€šæ€§
curl -I http://jenkins.cares-copilot.com
# åº”è¿”å› HTTP 200 æˆ– 403ï¼ˆè¯´æ˜èƒ½è®¿é—® Jenkinsï¼‰

# æµ‹è¯• Gitea Webhook endpoint
curl -I http://jenkins.cares-copilot.com/gitea-webhook/post
# åº”è¿”å› HTTP 405ï¼ˆè¯´æ˜ endpoint å­˜åœ¨ï¼‰
```



